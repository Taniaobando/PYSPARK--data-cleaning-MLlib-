{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conocimiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando librerias\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql import SparkSession, DataFrameStatFunctions, DataFrameNaFunctions\n",
    "from pyspark.sql.functions import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------#\n",
    "#Calcula la correlación de todas las variables de un dataframe\n",
    "#Entradas  dataframe: Dataframe de Spark\n",
    "#          headings: encabezado del dataframe (nombre de cada columna)\n",
    "#Salidas   corr: Lista de listas con la correlación de todas las variables \n",
    "def correlation(dataframe,headings):\n",
    "    n = len(headings)\n",
    "    corr = [[1 for _ in range(n)] for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            corr[i][j] = dataframe.corr(headings[i],headings[j])\n",
    "            corr[j][i] = corr[i][j]\n",
    "    return corr\n",
    "#----------------------------------------------------------------------------#\n",
    "\n",
    "#Encabezado del dataframe\n",
    "headings = ['school','sex','age','address','famsize','Pstatus','Medu','Fedu','Mjob',\n",
    " 'Fjob','reason','guardian','traveltime','studytime','failures','schoolsup',\n",
    " 'famsup','paid','activities','nursery','higher','internet','romantic',\n",
    " 'famrel','freetime','goout','Dalc','Walc','health','absences','G1','G2','G3']\n",
    "spark = SparkSession.builder.appName(\"Student\").getOrCreate()\n",
    "\n",
    "#Crear dataframe\n",
    "df=spark.read.csv('student-por.csv',sep=';',header=True)\n",
    "#df.show()\n",
    "\n",
    "#Tamaño del dataset\n",
    "#print(df.count())\n",
    "#El dataframe tiene 649 registros\n",
    "\n",
    "#Tipos de dato de cada variable\n",
    "#print(df.dtypes)\n",
    "#Todos los datos del dataframe inicial son de tipo string\n",
    "\n",
    "#----------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Null data\\ntotal_null = df.filter(\"school is null\").count() + df.filter(\"sex is null\").count() + df.filter(\"age is null\").count()\\ntotal_null+= df.filter(\"address is null\").count() + df.filter(\"famsize is null\").count() + df.filter(\"Pstatus is null\").count()\\ntotal_null+= df.filter(\"Medu is null\").count() + df.filter(\"Fedu is null\").count() + df.filter(\"Mjob is null\").count()\\ntotal_null+= df.filter(\"Fjob is null\").count() + df.filter(\"reason is null\").count() + df.filter(\"guardian is null\").count()\\ntotal_null+= df.filter(\"traveltime is null\").count() + df.filter(\"studytime is null\").count() + df.filter(\"failures is null\").count()\\ntotal_null+= df.filter(\"schoolsup is null\").count() + df.filter(\"famsup is null\").count() + df.filter(\"paid is null\").count()\\ntotal_null+= df.filter(\"activities is null\").count() + df.filter(\"nursery is null\").count() + df.filter(\"higher is null\").count()\\ntotal_null+= df.filter(\"internet is null\").count() + df.filter(\"romantic is null\").count() + df.filter(\"famrel is null\").count()\\ntotal_null+= df.filter(\"freetime is null\").count() + df.filter(\"goout is null\").count() + df.filter(\"Dalc is null\").count()\\ntotal_null+= df.filter(\"Walc is null\").count() + df.filter(\"health is null\").count() + df.filter(\"absences is null\").count()\\ntotal_null+= df.filter(\"G1 is null\").count() + df.filter(\"G2 is null\").count() + df.filter(\"G3 is null\").count()\\nprint(total_null)\\n#Encontramos que el dataframe inicial no tiene datos faltantes\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Null data\n",
    "total_null = df.filter(\"school is null\").count() + df.filter(\"sex is null\").count() + df.filter(\"age is null\").count()\n",
    "total_null+= df.filter(\"address is null\").count() + df.filter(\"famsize is null\").count() + df.filter(\"Pstatus is null\").count()\n",
    "total_null+= df.filter(\"Medu is null\").count() + df.filter(\"Fedu is null\").count() + df.filter(\"Mjob is null\").count()\n",
    "total_null+= df.filter(\"Fjob is null\").count() + df.filter(\"reason is null\").count() + df.filter(\"guardian is null\").count()\n",
    "total_null+= df.filter(\"traveltime is null\").count() + df.filter(\"studytime is null\").count() + df.filter(\"failures is null\").count()\n",
    "total_null+= df.filter(\"schoolsup is null\").count() + df.filter(\"famsup is null\").count() + df.filter(\"paid is null\").count()\n",
    "total_null+= df.filter(\"activities is null\").count() + df.filter(\"nursery is null\").count() + df.filter(\"higher is null\").count()\n",
    "total_null+= df.filter(\"internet is null\").count() + df.filter(\"romantic is null\").count() + df.filter(\"famrel is null\").count()\n",
    "total_null+= df.filter(\"freetime is null\").count() + df.filter(\"goout is null\").count() + df.filter(\"Dalc is null\").count()\n",
    "total_null+= df.filter(\"Walc is null\").count() + df.filter(\"health is null\").count() + df.filter(\"absences is null\").count()\n",
    "total_null+= df.filter(\"G1 is null\").count() + df.filter(\"G2 is null\").count() + df.filter(\"G3 is null\").count()\n",
    "print(total_null)\n",
    "#Encontramos que el dataframe inicial no tiene datos faltantes\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Casteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------#\n",
    "#Reemplazar valores categoricos a númericos\n",
    "df = df.withColumn(\"school\", regexp_replace(\"school\", \"GP\", \"0\"))\n",
    "df = df.withColumn(\"school\", regexp_replace(\"school\", \"MS\", \"1\"))\n",
    "df = df.withColumn(\"sex\", regexp_replace(\"sex\", \"F\", \"0\"))\n",
    "df = df.withColumn(\"sex\", regexp_replace(\"sex\", \"M\", \"1\"))\n",
    "df = df.withColumn(\"address\", regexp_replace(\"address\", \"R\", \"0\"))\n",
    "df = df.withColumn(\"address\", regexp_replace(\"address\", \"U\", \"1\"))\n",
    "df = df.withColumn(\"famsize\", regexp_replace(\"famsize\", \"LE3\", \"0\"))\n",
    "df = df.withColumn(\"famsize\", regexp_replace(\"famsize\", \"GT3\", \"1\"))\n",
    "df = df.withColumn(\"Pstatus\", regexp_replace(\"Pstatus\", \"A\", \"0\"))\n",
    "df = df.withColumn(\"Pstatus\", regexp_replace(\"Pstatus\", \"T\", \"1\"))\n",
    "df = df.withColumn(\"Mjob\", regexp_replace(\"Mjob\", \"other\", \"0\"))\n",
    "df = df.withColumn(\"Mjob\", regexp_replace(\"Mjob\", \"at_home\", \"1\"))\n",
    "df = df.withColumn(\"Mjob\", regexp_replace(\"Mjob\", \"teacher\", \"2\"))\n",
    "df = df.withColumn(\"Mjob\", regexp_replace(\"Mjob\", \"services\", \"3\"))\n",
    "df = df.withColumn(\"Mjob\", regexp_replace(\"Mjob\", \"health\", \"4\"))\n",
    "df = df.withColumn(\"Fjob\", regexp_replace(\"Fjob\", \"other\", \"0\"))\n",
    "df = df.withColumn(\"Fjob\", regexp_replace(\"Fjob\", \"at_home\", \"1\"))\n",
    "df = df.withColumn(\"Fjob\", regexp_replace(\"Fjob\", \"teacher\", \"2\"))\n",
    "df = df.withColumn(\"Fjob\", regexp_replace(\"Fjob\", \"services\", \"3\"))\n",
    "df = df.withColumn(\"Fjob\", regexp_replace(\"Fjob\", \"health\", \"4\"))\n",
    "df = df.withColumn(\"reason\", regexp_replace(\"reason\", \"other\", \"0\"))\n",
    "df = df.withColumn(\"reason\", regexp_replace(\"reason\", \"home\", \"1\"))\n",
    "df = df.withColumn(\"reason\", regexp_replace(\"reason\", \"reputation\", \"2\"))\n",
    "df = df.withColumn(\"reason\", regexp_replace(\"reason\", \"course\", \"3\"))\n",
    "df = df.withColumn(\"guardian\", regexp_replace(\"guardian\", \"father\", \"1\"))\n",
    "df = df.withColumn(\"guardian\", regexp_replace(\"guardian\", \"mother\", \"2\"))\n",
    "df = df.withColumn(\"guardian\", regexp_replace(\"guardian\", \"other\", \"0\"))\n",
    "df = df.withColumn(\"schoolsup\", regexp_replace(\"schoolsup\", \"no\", \"0\"))\n",
    "df = df.withColumn(\"schoolsup\", regexp_replace(\"schoolsup\", \"yes\", \"1\"))\n",
    "df = df.withColumn(\"famsup\", regexp_replace(\"famsup\", \"no\", \"0\"))\n",
    "df = df.withColumn(\"famsup\", regexp_replace(\"famsup\", \"yes\", \"1\"))\n",
    "df = df.withColumn(\"paid\", regexp_replace(\"paid\", \"no\", \"0\"))\n",
    "df = df.withColumn(\"paid\", regexp_replace(\"paid\", \"yes\", \"1\"))\n",
    "df = df.withColumn(\"activities\", regexp_replace(\"activities\", \"no\", \"0\"))\n",
    "df = df.withColumn(\"activities\", regexp_replace(\"activities\", \"yes\", \"1\"))\n",
    "df = df.withColumn(\"nursery\", regexp_replace(\"nursery\", \"no\", \"0\"))\n",
    "df = df.withColumn(\"nursery\", regexp_replace(\"nursery\", \"yes\", \"1\"))\n",
    "df = df.withColumn(\"higher\", regexp_replace(\"higher\", \"no\", \"0\"))\n",
    "df = df.withColumn(\"higher\", regexp_replace(\"higher\", \"yes\", \"1\"))\n",
    "df = df.withColumn(\"internet\", regexp_replace(\"internet\", \"no\", \"0\"))\n",
    "df = df.withColumn(\"internet\", regexp_replace(\"internet\", \"yes\", \"1\"))\n",
    "df = df.withColumn(\"romantic\", regexp_replace(\"romantic\", \"no\", \"0\"))\n",
    "df = df.withColumn(\"romantic\", regexp_replace(\"romantic\", \"yes\", \"1\"))\n",
    "#df.show()\n",
    "#----------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------#\n",
    "#Casteo de todos los datos de string a int\n",
    "df = df.withColumn('school', df.school.astype(\"int\"))\n",
    "df = df.withColumn('sex', df.sex.astype(\"int\"))\n",
    "df = df.withColumn('age', df.age.astype(\"int\"))\n",
    "df = df.withColumn('address', df.address.astype(\"int\"))\n",
    "df = df.withColumn('famsize', df.famsize.astype(\"int\"))\n",
    "df = df.withColumn('Pstatus', df.Pstatus.astype(\"int\"))\n",
    "df = df.withColumn('Medu', df.Medu.astype(\"int\"))\n",
    "df = df.withColumn('Fedu', df.Fedu.astype(\"int\"))\n",
    "df = df.withColumn('Mjob', df.Mjob.astype(\"int\"))\n",
    "df = df.withColumn('Fjob', df.Fjob.astype(\"int\"))\n",
    "df = df.withColumn('reason', df.reason.astype(\"int\"))\n",
    "df = df.withColumn('guardian', df.guardian.astype(\"int\"))\n",
    "df = df.withColumn('traveltime', df.traveltime.astype(\"int\"))\n",
    "df = df.withColumn('studytime', df.studytime.astype(\"int\"))\n",
    "df = df.withColumn('failures', df.failures.astype(\"int\"))\n",
    "df = df.withColumn('schoolsup', df.schoolsup.astype(\"int\"))\n",
    "df = df.withColumn('famsup', df.famsup.astype(\"int\"))\n",
    "df = df.withColumn('paid', df.paid.astype(\"int\"))\n",
    "df = df.withColumn('activities', df.activities.astype(\"int\"))\n",
    "df = df.withColumn('nursery', df.nursery.astype(\"int\"))\n",
    "df = df.withColumn('higher', df.higher.astype(\"int\"))\n",
    "df = df.withColumn('internet', df.internet.astype(\"int\"))\n",
    "df = df.withColumn('romantic', df.romantic.astype(\"int\"))\n",
    "df = df.withColumn('famrel', df.famrel.astype(\"int\"))\n",
    "df = df.withColumn('freetime', df.freetime.astype(\"int\"))\n",
    "df = df.withColumn('goout', df.goout.astype(\"int\"))\n",
    "df = df.withColumn('Dalc', df.Dalc.astype(\"int\"))\n",
    "df = df.withColumn('Walc', df.Walc.astype(\"int\"))\n",
    "df = df.withColumn('health', df.health.astype(\"int\"))\n",
    "df = df.withColumn('absences', df.absences.astype(\"int\"))\n",
    "df = df.withColumn('G1', df.G1.astype(\"int\"))\n",
    "df = df.withColumn('G2', df.G2.astype(\"int\"))\n",
    "df = df.withColumn('G3', df.G3.astype(\"int\"))\n",
    "#df.show()\n",
    "#----------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------#\n",
    "\"\"\"\n",
    "Aquí defininimos un umbral del 60% de la nota máxima para\n",
    "establecer quienes aprueban y quienes reprueban.\n",
    "\n",
    "Nota: Es importante hacer un casteo luego de unir la partición de los datasets,\n",
    "obtuvimos algunos errores por omitir esto.\n",
    "\"\"\"\n",
    "\n",
    "#Estableciendo umbral para el primer periodo\n",
    "df = df.withColumn('G1', df.G1.astype(\"int\"))\n",
    "approved = df.filter(df.G1 >= 12)\n",
    "reproved = df.filter(df.G1 < 12)\n",
    "for i in range(12):\n",
    "    reproved = reproved.withColumn(\"G1\", regexp_replace(\"G1\", \"{}\".format(i), \"0\"))\n",
    "for i in range(12,20):\n",
    "    approved = approved.withColumn(\"G1\", regexp_replace(\"G1\", \"{}\".format(i), \"1\"))\n",
    "\n",
    "df = approved.union(reproved)\n",
    "df = df.withColumn('G1', df.G1.astype(\"int\"))\n",
    "\n",
    "#Estableciendo umbral para el segundo periodo\n",
    "df = df.withColumn('G2', df.G2.astype(\"int\"))\n",
    "approved = df.filter(df.G2 >= 12)\n",
    "reproved = df.filter(df.G2 < 12)\n",
    "for i in range(12):\n",
    "    reproved = reproved.withColumn(\"G2\", regexp_replace(\"G2\", \"{}\".format(i), \"0\"))\n",
    "for i in range(12,20):\n",
    "    approved = approved.withColumn(\"G2\", regexp_replace(\"G2\", \"{}\".format(i), \"1\"))\n",
    "df = approved.union(reproved)\n",
    "df = df.withColumn('G2', df.G2.astype(\"int\"))\n",
    "\n",
    "#Estableciendo umbral para el tercer periodo\n",
    "df = df.withColumn('G3', df.G3.astype(\"int\"))\n",
    "approved = df.filter(df.G3 >= 12)\n",
    "reproved = df.filter(df.G3 < 12)\n",
    "for i in range(12):\n",
    "    reproved = reproved.withColumn(\"G3\", regexp_replace(\"G3\", \"{}\".format(i), \"0\"))\n",
    "for i in range(12,20):\n",
    "    approved = approved.withColumn(\"G3\", regexp_replace(\"G3\", \"{}\".format(i), \"1\"))\n",
    "    \n",
    "#print('Número de estudiantes que rerobaron:', reproved.count())\n",
    "#print('Número de estudiantes que aprobaron:', approved.count())\n",
    "#Aquí obtuvimos 301 estudiantes reprobados y 348 estudiantes aprobados\n",
    "\n",
    "df = approved.union(reproved)\n",
    "df = df.withColumn('G3', df.G3.astype(\"int\"))\n",
    "\n",
    "#df.count()\n",
    "#df.show()\n",
    "#----------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------#\n",
    "#Ver medidas de centralidad\n",
    "#df.describe().toPandas()\n",
    "#df.toPandas().mode()\n",
    "#----------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nfor x in headings:\\n    print(x)\\n    plt.boxplot(df.toPandas()[x],vert = 0)\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------#\n",
    "#Visualizar datos atipicos\n",
    "\"\"\"\"\n",
    "for x in headings:\n",
    "    print(x)\n",
    "    plt.boxplot(df.toPandas()[x],vert = 0)\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "#----------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\natypic_age_22=df.filter(df[\\'age\\'] == 22).count()\\natypic_p_status_0=df.filter(df[\\'Pstatus\\'] == 0).count()#viven padres juntos o separados \\natypic_travel_time_4=df.filter(df[\\'traveltime\\'] == 4).count()#tiempo de la casa a el colegio\\natypic_studytime_4=df.filter(df[\\'studytime\\'] == 4).count()#tiempo de estudio\\natypic_failures_1=df.filter(df[\\'failures\\'] == 1).count()#número de fallos de clases anteriores\\natypic_failures_2=df.filter(df[\\'failures\\'] == 2).count()\\natypic_failures_3=df.filter(df[\\'failures\\'] == 3).count() \\natypic_schoolsup_1=df.filter(df[\\'schoolsup\\'] == 1).count()#apoyo educativo adicional\\natypic_paid_1=df.filter(df[\\'paid\\'] == 1).count()#clases extra pagadas dentro de la asignatura del curso (portugués)\\natypic_nursery_0=df.filter(df[\\'nursery\\'] == 0).count()#asistio a la guarderia\\natypic_higher_0=df.filter(df[\\'higher\\'] == 0).count()#piensa  cursar estudios superiores\\natypic_internet_0=df.filter(df[\\'internet\\'] == 0).count()\\natypic_famrel_1=df.filter(df[\\'famrel\\'] == 1).count()#calidad de las relaciones familiares\\natypic_famrel_2=df.filter(df[\\'famrel\\'] == 2).count()\\natypic_freetime_1=df.filter(df[\\'freetime\\'] == 1).count()#tiempo libre despues de la escuela\\natypic_Dalc_4=df.filter(df[\\'Dalc\\'] == 4).count()# consumo de alcohol entre semana\\natypic_Dalc_5=df.filter(df[\\'Dalc\\'] == 5).count()\\natypic_absences=df.filter(df[\\'absences\\'] > 16).count()#numero de ausencias escolares\\n\\nprint(\"atypic_age_22:\",atypic_age_22)\\nprint(\"atypic_p_status_0:\",atypic_p_status_0)\\nprint(\"atypic_travel_time_4:\",atypic_travel_time_4)\\nprint(\"atypic_studytime_4:\",atypic_studytime_4)\\nprint(\"atypic_failures_1:\",atypic_failures_1)\\nprint(\"atypic_failures_2:\",atypic_failures_2)\\nprint(\"atypic_failures_3:\",atypic_failures_3)\\nprint(\"atypic_schoolsup_1:\",atypic_schoolsup_1)\\nprint(\"atypic_paid_1:\",atypic_paid_1)\\nprint(\"atypic_nursery_0:\",atypic_nursery_0)\\nprint(\"atypic_higher_0:\",atypic_higher_0)\\nprint(\"atypic_internet_0:\",atypic_internet_0)\\nprint(\"atypic_famrel_1:\",atypic_famrel_1)\\nprint(\"atypic_famrel_2:\",atypic_famrel_2)\\nprint(\"atypic_freetime_1:\",atypic_freetime_1)\\nprint(\"atypic_Dalc_4:\",atypic_Dalc_4)\\nprint(\"atypic_Dalc_5:\",atypic_Dalc_5)\\nprint(\"atypic_absences:\",atypic_absences)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------#\n",
    "#cantidad de datos atipicos\n",
    "\"\"\"\n",
    "atypic_age_22=df.filter(df['age'] == 22).count()\n",
    "atypic_p_status_0=df.filter(df['Pstatus'] == 0).count()#viven padres juntos o separados \n",
    "atypic_travel_time_4=df.filter(df['traveltime'] == 4).count()#tiempo de la casa a el colegio\n",
    "atypic_studytime_4=df.filter(df['studytime'] == 4).count()#tiempo de estudio\n",
    "atypic_failures_1=df.filter(df['failures'] == 1).count()#número de fallos de clases anteriores\n",
    "atypic_failures_2=df.filter(df['failures'] == 2).count()\n",
    "atypic_failures_3=df.filter(df['failures'] == 3).count() \n",
    "atypic_schoolsup_1=df.filter(df['schoolsup'] == 1).count()#apoyo educativo adicional\n",
    "atypic_paid_1=df.filter(df['paid'] == 1).count()#clases extra pagadas dentro de la asignatura del curso (portugués)\n",
    "atypic_nursery_0=df.filter(df['nursery'] == 0).count()#asistio a la guarderia\n",
    "atypic_higher_0=df.filter(df['higher'] == 0).count()#piensa  cursar estudios superiores\n",
    "atypic_internet_0=df.filter(df['internet'] == 0).count()\n",
    "atypic_famrel_1=df.filter(df['famrel'] == 1).count()#calidad de las relaciones familiares\n",
    "atypic_famrel_2=df.filter(df['famrel'] == 2).count()\n",
    "atypic_freetime_1=df.filter(df['freetime'] == 1).count()#tiempo libre despues de la escuela\n",
    "atypic_Dalc_4=df.filter(df['Dalc'] == 4).count()# consumo de alcohol entre semana\n",
    "atypic_Dalc_5=df.filter(df['Dalc'] == 5).count()\n",
    "atypic_absences=df.filter(df['absences'] > 16).count()#numero de ausencias escolares\n",
    "\n",
    "print(\"atypic_age_22:\",atypic_age_22)\n",
    "print(\"atypic_p_status_0:\",atypic_p_status_0)\n",
    "print(\"atypic_travel_time_4:\",atypic_travel_time_4)\n",
    "print(\"atypic_studytime_4:\",atypic_studytime_4)\n",
    "print(\"atypic_failures_1:\",atypic_failures_1)\n",
    "print(\"atypic_failures_2:\",atypic_failures_2)\n",
    "print(\"atypic_failures_3:\",atypic_failures_3)\n",
    "print(\"atypic_schoolsup_1:\",atypic_schoolsup_1)\n",
    "print(\"atypic_paid_1:\",atypic_paid_1)\n",
    "print(\"atypic_nursery_0:\",atypic_nursery_0)\n",
    "print(\"atypic_higher_0:\",atypic_higher_0)\n",
    "print(\"atypic_internet_0:\",atypic_internet_0)\n",
    "print(\"atypic_famrel_1:\",atypic_famrel_1)\n",
    "print(\"atypic_famrel_2:\",atypic_famrel_2)\n",
    "print(\"atypic_freetime_1:\",atypic_freetime_1)\n",
    "print(\"atypic_Dalc_4:\",atypic_Dalc_4)\n",
    "print(\"atypic_Dalc_5:\",atypic_Dalc_5)\n",
    "print(\"atypic_absences:\",atypic_absences)\n",
    "\"\"\"\n",
    "\n",
    "#----------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------#\n",
    "\"\"\"\n",
    "Eliminación de datos atipicos\n",
    "Nota: Para esta fase establecimos que estabamos dispuestos a eliminar hasta un 10%\n",
    "del total de los datos del dataset (649).\n",
    "\"\"\"\n",
    "\n",
    "df=df.filter(df['age'] != 22)\n",
    "df=df.filter(df['traveltime'] != 4)\n",
    "df=df.filter(df['absences'] <17)\n",
    "df=df.filter(df['Dalc'] != 5)\n",
    "df.count()\n",
    "#df.count()\n",
    "#Al depurar los datos atípicos, terminamos con un total de 608 datos.\n",
    "\n",
    "#----------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\napproved = df.filter(df.G3 == 1)\\nreproved = df.filter(df.G3 == 0)\\nprint(\"cantidad final de estudiantes aprobados\",approved.count())\\nprint(\"cantidad final de estudiantes reprobados\",reproved.count())\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mirar balance de los datos\n",
    "\"\"\"\n",
    "approved = df.filter(df.G3 == 1)\n",
    "reproved = df.filter(df.G3 == 0)\n",
    "print(\"cantidad final de estudiantes aprobados\",approved.count())\n",
    "print(\"cantidad final de estudiantes reprobados\",reproved.count())\n",
    "\"\"\"\n",
    "#----------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicar un balanceo de los datos reduciendo la clase mayorataria\n",
    "approved=approved.sample(fraction=0.809,seed = 9403040)\n",
    "#print( \"approved\",approved.count())\n",
    "#----------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_assembler VectorAssembler_b7dbd096ab68\n",
      "+------+---+---+-------+-------+-------+----+----+----+----+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+--------------------+\n",
      "|school|sex|age|address|famsize|Pstatus|Medu|Fedu|Mjob|Fjob|reason|guardian|traveltime|studytime|failures|schoolsup|famsup|paid|activities|nursery|higher|internet|romantic|famrel|freetime|goout|Dalc|Walc|health|absences| G1| G2| G3|            features|\n",
      "+------+---+---+-------+-------+-------+----+----+----+----+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+--------------------+\n",
      "|     0|  0| 15|      1|      0|      1|   1|   1|   1|   0|     0|       2|         1|        2|       0|        1|     0|   0|         0|      1|     1|       1|       0|     4|       3|    2|   2|   3|     3|       6|  1|  1|  1|[0.0,0.0,15.0,1.0...|\n",
      "|     0|  0| 15|      1|      1|      1|   4|   2|   4|   3|     1|       2|         1|        3|       0|        0|     1|   0|         1|      1|     1|       1|       1|     3|       2|    2|   1|   1|     5|       0|  1|  1|  1|[0.0,0.0,15.0,1.0...|\n",
      "|     0|  1| 16|      1|      0|      1|   4|   3|   3|   0|     2|       2|         1|        2|       0|        0|     1|   0|         1|      1|     1|       1|       0|     5|       4|    2|   1|   2|     5|       6|  1|  1|  1|[0.0,1.0,16.0,1.0...|\n",
      "|     0|  1| 16|      1|      0|      1|   2|   2|   0|   0|     1|       2|         1|        2|       0|        0|     0|   0|         0|      1|     1|       1|       0|     4|       4|    4|   1|   1|     3|       0|  1|  1|  1|[0.0,1.0,16.0,1.0...|\n",
      "|     0|  1| 15|      1|      0|      0|   3|   2|   3|   0|     1|       2|         1|        2|       0|        0|     1|   0|         0|      1|     1|       1|       0|     4|       2|    2|   1|   1|     1|       0|  1|  1|  1|[0.0,1.0,15.0,1.0...|\n",
      "+------+---+---+-------+-------+-------+----+----+----+----+------+--------+----------+---------+--------+---------+------+----+----------+-------+------+--------+--------+------+--------+-----+----+----+------+--------+---+---+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+\n",
      "| G3|            features|\n",
      "+---+--------------------+\n",
      "|  1|[0.0,0.0,15.0,1.0...|\n",
      "|  1|[0.0,0.0,15.0,1.0...|\n",
      "|  1|[0.0,1.0,16.0,1.0...|\n",
      "|  1|[0.0,1.0,16.0,1.0...|\n",
      "|  1|[0.0,1.0,15.0,1.0...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+-------+\n",
      "| G3|            features|G3Index|\n",
      "+---+--------------------+-------+\n",
      "|  1|[0.0,0.0,15.0,1.0...|    0.0|\n",
      "|  1|[0.0,0.0,15.0,1.0...|    0.0|\n",
      "|  1|[0.0,1.0,16.0,1.0...|    0.0|\n",
      "|  1|[0.0,1.0,16.0,1.0...|    0.0|\n",
      "|  1|[0.0,1.0,15.0,1.0...|    0.0|\n",
      "+---+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+--------------------+-------+\n",
      "| G3|            features|G3Index|\n",
      "+---+--------------------+-------+\n",
      "|  1|(32,[0,2,5,6,7,10...|    0.0|\n",
      "|  1|(32,[0,2,6,7,8,10...|    0.0|\n",
      "|  1|(32,[2,4,6,7,10,1...|    0.0|\n",
      "|  1|(32,[2,5,6,7,10,1...|    0.0|\n",
      "|  1|[0.0,0.0,15.0,0.0...|    0.0|\n",
      "|  1|[0.0,0.0,15.0,1.0...|    0.0|\n",
      "|  1|[0.0,0.0,16.0,1.0...|    0.0|\n",
      "|  1|[0.0,0.0,16.0,1.0...|    0.0|\n",
      "|  1|[0.0,0.0,16.0,1.0...|    0.0|\n",
      "|  1|[0.0,0.0,16.0,1.0...|    0.0|\n",
      "|  1|[0.0,0.0,16.0,1.0...|    0.0|\n",
      "|  1|[0.0,0.0,16.0,1.0...|    0.0|\n",
      "|  1|[0.0,0.0,16.0,1.0...|    0.0|\n",
      "|  1|[0.0,0.0,17.0,0.0...|    0.0|\n",
      "|  1|[0.0,0.0,17.0,1.0...|    0.0|\n",
      "|  1|[0.0,0.0,17.0,1.0...|    0.0|\n",
      "|  1|[0.0,0.0,17.0,1.0...|    0.0|\n",
      "|  1|[0.0,0.0,17.0,1.0...|    0.0|\n",
      "|  1|[0.0,0.0,17.0,1.0...|    0.0|\n",
      "|  1|[0.0,0.0,17.0,1.0...|    0.0|\n",
      "+---+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+-------+\n",
      "|prediction|G3Index|\n",
      "+----------+-------+\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "|       0.0|    0.0|\n",
      "+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.04375 \n",
      "Accuracy = 0.95625 \n",
      "LogisticRegressionModel: uid=LogisticRegression_0e85326064b6, numClasses=2, numFeatures=32\n",
      "Coefficients: (32,[2,30,31],[0.03038773926470178,-0.18392108457905598,-1.1326352180425647])\n",
      "Intercept: -0.16229039528276185\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#RL\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=['school','sex','age','address','famsize','Pstatus','Medu','Fedu','Mjob',\n",
    " 'Fjob','reason','guardian','traveltime','studytime','failures','schoolsup',\n",
    " 'famsup','paid','activities','nursery','higher','internet','romantic',\n",
    " 'famrel','freetime','goout','Dalc','Walc','health','absences','G1','G2'], outputCol=\"features\")\n",
    "\n",
    "print(\"vector_assembler\",vector_assembler)\n",
    "\n",
    "df_temp = vector_assembler.transform(df)\n",
    "df_temp.show(5)\n",
    "# get dataframe with all necessary data in the appropriate form\n",
    "df = df_temp.drop('school','sex','age','address','famsize','Pstatus','Medu','Fedu','Mjob',\n",
    " 'Fjob','reason','guardian','traveltime','studytime','failures','schoolsup',\n",
    " 'famsup','paid','activities','nursery','higher','internet','romantic',\n",
    " 'famrel','freetime','goout','Dalc','Walc','health','absences','G1','G2')\n",
    "df.show(5)\n",
    "\n",
    "l_indexer = StringIndexer(inputCol=\"G3\",\n",
    "                          outputCol=\"G3Index\")\n",
    "\n",
    "df = l_indexer.fit(df).transform(df)\n",
    "\n",
    "df.show(5)\n",
    "\n",
    "# divide our data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "testData.show()\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"G3Index\", featuresCol=\"features\",maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "model = lr.fit(trainingData)\n",
    "\n",
    "# make predictions using our trained model\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# look at the result\n",
    "predictions.select(\"prediction\", \"G3Index\").show(5)\n",
    "\n",
    "# estimate the accuracy of the prediction\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"G3Index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "print(\"Accuracy = %g \" % accuracy)\n",
    "\n",
    "# print model summary\n",
    "print(model)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"school\" among (G3, features, G3Index);",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-d6c3e399cbbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Visualizar la correlación de las variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcorrelacion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorrelation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheadings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrelacion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquare\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#----------------------------------------------------------------------------#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-db27180e2c35>\u001b[0m in \u001b[0;36mcorrelation\u001b[1;34m(dataframe, headings)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mcorr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheadings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheadings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mcorr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcorr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcorr\u001b[1;34m(self, col1, col2, method)\u001b[0m\n\u001b[0;32m   2007\u001b[0m             raise ValueError(\"Currently only the calculation of the Pearson Correlation \" +\n\u001b[0;32m   2008\u001b[0m                              \"coefficient is supported.\")\n\u001b[1;32m-> 2009\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2011\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Cannot resolve column name \"school\" among (G3, features, G3Index);"
     ]
    }
   ],
   "source": [
    "#Visualizar la correlación de las variables\n",
    "correlacion = correlation(df,headings)\n",
    "sns.heatmap(correlacion, square=True)\n",
    "#----------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analizar correlaciones a partir del mapa de calor de correlaciones\n",
    "\"\"\"\n",
    "#Correlaciones positivamente fuertes\n",
    "print('Correlación entre G1 y G2:', df.corr('G1','G2'))\n",
    "print('Correlación entre G1 y G3:', df.corr('G1','G3'))\n",
    "print('Correlación entre G2 y G3:', df.corr('G2','G3'))\n",
    "\n",
    "#Correlaciones positivamente moderadas\n",
    "print('Correlación entre Medu y Fedu:', df.corr('Medu','Fedu'))\n",
    "print('Correlación entre Walc y Dalc:', df.corr('Walc','Dalc'))\n",
    "\n",
    "#Correlaciones negativamente moderadas\n",
    "print('Correlación entre school y address:', df.corr('school','address'))\n",
    "print('Correlación entre traveltime y address:', df.corr('traveltime','address'))\n",
    "print('Correlación entre failures y G1:', df.corr('failures','G1'))\n",
    "print('Correlación entre failures y G2:', df.corr('failures','G2'))\n",
    "print('Correlación entre failures y G3:', df.corr('failures','G3'))\n",
    "\"\"\"\n",
    "#----------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creación de los datasets finales\n",
    "final_df_with_corr = df.drop('G2')\n",
    "final_df = final_df_with_corr.drop('G1')\n",
    "#----------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finaliza la sesión de spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
