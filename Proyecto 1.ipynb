{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin G1 y G2\n",
      "\n",
      "\n",
      "logistic_Regression\n",
      "Accuracy = 0.7094972067039106\n",
      "Precision = 0.7640449438202247\n",
      "Recall = 0.6868686868686869\n",
      "F1 score = 0.7103152683871599\n",
      "Area under ROC curve = 0.7121843434343434\n",
      "\n",
      "\n",
      "Sin G1 y G2\n",
      "\n",
      "\n",
      "random_Forest\n",
      "Accuracy = 0.6815642458100558\n",
      "Precision = 0.7763157894736842\n",
      "Recall = 0.5959595959595959\n",
      "F1 score = 0.6806494576949746\n",
      "Area under ROC curve = 0.6917297979797981\n",
      "Sin G1 y G2\n",
      "\n",
      "\n",
      "mvs\n",
      "Accuracy = 0.7262569832402235\n",
      "Precision = 0.7155172413793104\n",
      "Recall = 0.8383838383838383\n",
      "F1 score = 0.7208079435180904\n",
      "Area under ROC curve = 0.7129419191919191\n",
      "\n",
      "\n",
      "#------------------------------------------------------------------------------------------#\n",
      "Sin G2\n",
      "\n",
      "\n",
      "logistic_Regression\n",
      "Accuracy = 0.8666666666666667\n",
      "Precision = 0.875\n",
      "Recall = 0.875\n",
      "F1 score = 0.8666666666666667\n",
      "Area under ROC curve = 0.8660714285714286\n",
      "\n",
      "\n",
      "Sin G2\n",
      "\n",
      "\n",
      "random_Forest\n",
      "Accuracy = 0.8848484848484849\n",
      "Precision = 0.8631578947368421\n",
      "Recall = 0.9318181818181818\n",
      "F1 score = 0.8843091334894613\n",
      "Area under ROC curve = 0.8814935064935066\n",
      "Sin G2\n",
      "\n",
      "\n",
      "mvs\n",
      "Accuracy = 0.8848484848484849\n",
      "Precision = 0.8631578947368421\n",
      "Recall = 0.9318181818181818\n",
      "F1 score = 0.8843091334894613\n",
      "Area under ROC curve = 0.8814935064935066\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------#\n",
    "#       Procesamiento de grandes volumenes de datos 2020-2                   #\n",
    "#               Proyecto 1 (data cleaning + MLlib)                           #\n",
    "#                       Alejandro Ayala Gil                                  #\n",
    "#                       Esteban Cardona Gil                                  #\n",
    "#                    Juan Camilo Gomez Muñoz                                 #\n",
    "#                        Julian Paredes C                                    #\n",
    "#                    Tania C. Obando Suárez                                  #\n",
    "#----------------------------------------------------------------------------#\n",
    "\n",
    "#Importando librerias\n",
    "import findspark \n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql import SparkSession, DataFrameStatFunctions, DataFrameNaFunctions\n",
    "from pyspark.sql.functions import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "#------------------------FUNCIONES AUXILIARES----------------------------------#\n",
    "\n",
    "def correlation(dataframe,headings):\n",
    "    n = len(headings)\n",
    "    corr = [[1 for _ in range(n)] for _ in range(n)]\n",
    "    dic=dict()\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            corr[i][j] = dataframe.corr(headings[i],headings[j])\n",
    "            corr[j][i] = corr[i][j]\n",
    "            if corr[i][j]>0.70:\n",
    "                dic[(headings[i],headings[j])]=corr[i][j]\n",
    "    print(dic)\n",
    "    \n",
    "    #Analizar correlaciones a partir del mapa de calor de correlaciones\n",
    "\n",
    "    \"\"\"\n",
    "    #Correlaciones positivamente fuertes\n",
    "    print('Correlación entre G1 y G2:', df.corr('G1','G2'))\n",
    "    print('Correlación entre G1 y G3:', df.corr('G1','G3'))\n",
    "    print('Correlación entre G2 y G3:', df.corr('G2','G3'))\n",
    "\n",
    "    #Correlaciones positivamente moderadas\n",
    "    print('Correlación entre Medu y Fedu:', df.corr('Medu','Fedu'))\n",
    "    print('Correlación entre Walc y Dalc:', df.corr('Walc','Dalc'))\n",
    "\n",
    "    #Correlaciones negativamente moderadas\n",
    "    print('Correlación entre school y address:', df.corr('school','address'))\n",
    "    print('Correlación entre traveltime y address:', df.corr('traveltime','address'))\n",
    "    print('Correlación entre failures y G1:', df.corr('failures','G1'))\n",
    "    print('Correlación entre failures y G2:', df.corr('failures','G2'))\n",
    "    print('Correlación entre failures y G3:', df.corr('failures','G3'))\n",
    "    \"\"\"          \n",
    "    return corr\n",
    "\n",
    "#-----------------CONOCIMIENTO Y LIMPIEZA DE LOS DATOS------------------------#\n",
    "\n",
    "def  nullData(df):\n",
    "    #Datos nulos\n",
    "    total_null = df.filter(\"school is null\").count() + df.filter(\"sex is null\").count() + df.filter(\"age is null\").count()\n",
    "    total_null+= df.filter(\"address is null\").count() + df.filter(\"famsize is null\").count() + df.filter(\"Pstatus is null\").count()\n",
    "    total_null+= df.filter(\"Medu is null\").count() + df.filter(\"Fedu is null\").count() + df.filter(\"Mjob is null\").count()\n",
    "    total_null+= df.filter(\"Fjob is null\").count() + df.filter(\"reason is null\").count() + df.filter(\"guardian is null\").count()\n",
    "    total_null+= df.filter(\"traveltime is null\").count() + df.filter(\"studytime is null\").count() + df.filter(\"failures is null\").count()\n",
    "    total_null+= df.filter(\"schoolsup is null\").count() + df.filter(\"famsup is null\").count() + df.filter(\"paid is null\").count()\n",
    "    total_null+= df.filter(\"activities is null\").count() + df.filter(\"nursery is null\").count() + df.filter(\"higher is null\").count()\n",
    "    total_null+= df.filter(\"internet is null\").count() + df.filter(\"romantic is null\").count() + df.filter(\"famrel is null\").count()\n",
    "    total_null+= df.filter(\"freetime is null\").count() + df.filter(\"goout is null\").count() + df.filter(\"Dalc is null\").count()\n",
    "    total_null+= df.filter(\"Walc is null\").count() + df.filter(\"health is null\").count() + df.filter(\"absences is null\").count()\n",
    "    total_null+= df.filter(\"G1 is null\").count() + df.filter(\"G2 is null\").count() + df.filter(\"G3 is null\").count()\n",
    "    print(total_null,\"total_null\")\n",
    "    return(total_null)\n",
    "    #Encontramos que el dataframe inicial no tiene datos faltantes o datos nulos.\n",
    "\n",
    "def categoricalToNumerical(df):\n",
    "\n",
    "    #Reemplazar valores categoricos a númericos\n",
    "    df = df.withColumn(\"school\", regexp_replace(\"school\", \"GP\", \"0\"))\n",
    "    df = df.withColumn(\"school\", regexp_replace(\"school\", \"MS\", \"1\"))\n",
    "    df = df.withColumn(\"sex\", regexp_replace(\"sex\", \"F\", \"0\"))\n",
    "    df = df.withColumn(\"sex\", regexp_replace(\"sex\", \"M\", \"1\"))\n",
    "    df = df.withColumn(\"address\", regexp_replace(\"address\", \"R\", \"0\"))\n",
    "    df = df.withColumn(\"address\", regexp_replace(\"address\", \"U\", \"1\"))\n",
    "    df = df.withColumn(\"famsize\", regexp_replace(\"famsize\", \"LE3\", \"0\"))\n",
    "    df = df.withColumn(\"famsize\", regexp_replace(\"famsize\", \"GT3\", \"1\"))\n",
    "    df = df.withColumn(\"Pstatus\", regexp_replace(\"Pstatus\", \"A\", \"0\"))\n",
    "    df = df.withColumn(\"Pstatus\", regexp_replace(\"Pstatus\", \"T\", \"1\"))\n",
    "    df = df.withColumn(\"Mjob\", regexp_replace(\"Mjob\", \"other\", \"0\"))\n",
    "    df = df.withColumn(\"Mjob\", regexp_replace(\"Mjob\", \"at_home\", \"1\"))\n",
    "    df = df.withColumn(\"Mjob\", regexp_replace(\"Mjob\", \"teacher\", \"2\"))\n",
    "    df = df.withColumn(\"Mjob\", regexp_replace(\"Mjob\", \"services\", \"3\"))\n",
    "    df = df.withColumn(\"Mjob\", regexp_replace(\"Mjob\", \"health\", \"4\"))\n",
    "    df = df.withColumn(\"Fjob\", regexp_replace(\"Fjob\", \"other\", \"0\"))\n",
    "    df = df.withColumn(\"Fjob\", regexp_replace(\"Fjob\", \"at_home\", \"1\"))\n",
    "    df = df.withColumn(\"Fjob\", regexp_replace(\"Fjob\", \"teacher\", \"2\"))\n",
    "    df = df.withColumn(\"Fjob\", regexp_replace(\"Fjob\", \"services\", \"3\"))\n",
    "    df = df.withColumn(\"Fjob\", regexp_replace(\"Fjob\", \"health\", \"4\"))\n",
    "    df = df.withColumn(\"reason\", regexp_replace(\"reason\", \"other\", \"0\"))\n",
    "    df = df.withColumn(\"reason\", regexp_replace(\"reason\", \"home\", \"1\"))\n",
    "    df = df.withColumn(\"reason\", regexp_replace(\"reason\", \"reputation\", \"2\"))\n",
    "    df = df.withColumn(\"reason\", regexp_replace(\"reason\", \"course\", \"3\"))\n",
    "    df = df.withColumn(\"guardian\", regexp_replace(\"guardian\", \"father\", \"1\"))\n",
    "    df = df.withColumn(\"guardian\", regexp_replace(\"guardian\", \"mother\", \"2\"))\n",
    "    df = df.withColumn(\"guardian\", regexp_replace(\"guardian\", \"other\", \"0\"))\n",
    "    df = df.withColumn(\"schoolsup\", regexp_replace(\"schoolsup\", \"no\", \"0\"))\n",
    "    df = df.withColumn(\"schoolsup\", regexp_replace(\"schoolsup\", \"yes\", \"1\"))\n",
    "    df = df.withColumn(\"famsup\", regexp_replace(\"famsup\", \"no\", \"0\"))\n",
    "    df = df.withColumn(\"famsup\", regexp_replace(\"famsup\", \"yes\", \"1\"))\n",
    "    df = df.withColumn(\"paid\", regexp_replace(\"paid\", \"no\", \"0\"))\n",
    "    df = df.withColumn(\"paid\", regexp_replace(\"paid\", \"yes\", \"1\"))\n",
    "    df = df.withColumn(\"activities\", regexp_replace(\"activities\", \"no\", \"0\"))\n",
    "    df = df.withColumn(\"activities\", regexp_replace(\"activities\", \"yes\", \"1\"))\n",
    "    df = df.withColumn(\"nursery\", regexp_replace(\"nursery\", \"no\", \"0\"))\n",
    "    df = df.withColumn(\"nursery\", regexp_replace(\"nursery\", \"yes\", \"1\"))\n",
    "    df = df.withColumn(\"higher\", regexp_replace(\"higher\", \"no\", \"0\"))\n",
    "    df = df.withColumn(\"higher\", regexp_replace(\"higher\", \"yes\", \"1\"))\n",
    "    df = df.withColumn(\"internet\", regexp_replace(\"internet\", \"no\", \"0\"))\n",
    "    df = df.withColumn(\"internet\", regexp_replace(\"internet\", \"yes\", \"1\"))\n",
    "    df = df.withColumn(\"romantic\", regexp_replace(\"romantic\", \"no\", \"0\"))\n",
    "    df = df.withColumn(\"romantic\", regexp_replace(\"romantic\", \"yes\", \"1\"))\n",
    "    #df.show()\n",
    "    return(df)\n",
    "\n",
    "def stringToInt(df):\n",
    "\n",
    "    #Casteo de todos los datos de string a int\n",
    "    df = df.withColumn('school', df.school.astype(\"int\"))\n",
    "    df = df.withColumn('sex', df.sex.astype(\"int\"))\n",
    "    df = df.withColumn('age', df.age.astype(\"int\"))\n",
    "    df = df.withColumn('address', df.address.astype(\"int\"))\n",
    "    df = df.withColumn('famsize', df.famsize.astype(\"int\"))\n",
    "    df = df.withColumn('Pstatus', df.Pstatus.astype(\"int\"))\n",
    "    df = df.withColumn('Medu', df.Medu.astype(\"int\"))\n",
    "    df = df.withColumn('Fedu', df.Fedu.astype(\"int\"))\n",
    "    df = df.withColumn('Mjob', df.Mjob.astype(\"int\"))\n",
    "    df = df.withColumn('Fjob', df.Fjob.astype(\"int\"))\n",
    "    df = df.withColumn('reason', df.reason.astype(\"int\"))\n",
    "    df = df.withColumn('guardian', df.guardian.astype(\"int\"))\n",
    "    df = df.withColumn('traveltime', df.traveltime.astype(\"int\"))\n",
    "    df = df.withColumn('studytime', df.studytime.astype(\"int\"))\n",
    "    df = df.withColumn('failures', df.failures.astype(\"int\"))\n",
    "    df = df.withColumn('schoolsup', df.schoolsup.astype(\"int\"))\n",
    "    df = df.withColumn('famsup', df.famsup.astype(\"int\"))\n",
    "    df = df.withColumn('paid', df.paid.astype(\"int\"))\n",
    "    df = df.withColumn('activities', df.activities.astype(\"int\"))\n",
    "    df = df.withColumn('nursery', df.nursery.astype(\"int\"))\n",
    "    df = df.withColumn('higher', df.higher.astype(\"int\"))\n",
    "    df = df.withColumn('internet', df.internet.astype(\"int\"))\n",
    "    df = df.withColumn('romantic', df.romantic.astype(\"int\"))\n",
    "    df = df.withColumn('famrel', df.famrel.astype(\"int\"))\n",
    "    df = df.withColumn('freetime', df.freetime.astype(\"int\"))\n",
    "    df = df.withColumn('goout', df.goout.astype(\"int\"))\n",
    "    df = df.withColumn('Dalc', df.Dalc.astype(\"int\"))\n",
    "    df = df.withColumn('Walc', df.Walc.astype(\"int\"))\n",
    "    df = df.withColumn('health', df.health.astype(\"int\"))\n",
    "    df = df.withColumn('absences', df.absences.astype(\"int\"))\n",
    "    df = df.withColumn('G1', df.G1.astype(\"int\"))\n",
    "    df = df.withColumn('G2', df.G2.astype(\"int\"))\n",
    "    df = df.withColumn('G3', df.G3.astype(\"int\"))\n",
    "    #df.show()\n",
    "    return(df)\n",
    "\n",
    "def approvedOrReproved(df):\n",
    "    \"\"\"\n",
    "    Aquí defininimos un umbral del 60% de la nota máxima para\n",
    "    establecer quienes aprueban y quienes reprueban.\n",
    "\n",
    "    Nota: Es importante hacer un casteo luego de unir la partición de los datasets,\n",
    "    obtuvimos algunos errores por omitir esto.\n",
    "    \"\"\"\n",
    "\n",
    "    #Estableciendo umbral para el primer periodo\n",
    "    df = df.withColumn('G1', df.G1.astype(\"int\"))\n",
    "    approved = df.filter(df.G1 >= 12)\n",
    "    reproved = df.filter(df.G1 < 12)\n",
    "    for i in range(12):\n",
    "        reproved = reproved.withColumn(\"G1\", regexp_replace(\"G1\", \"{}\".format(i), \"0\"))\n",
    "    for i in range(12,20):\n",
    "        approved = approved.withColumn(\"G1\", regexp_replace(\"G1\", \"{}\".format(i), \"1\"))\n",
    "\n",
    "    df = approved.union(reproved)\n",
    "    df = df.withColumn('G1', df.G1.astype(\"int\"))\n",
    "\n",
    "    #Estableciendo umbral para el segundo periodo\n",
    "    df = df.withColumn('G2', df.G2.astype(\"int\"))\n",
    "    approved = df.filter(df.G2 >= 12)\n",
    "    reproved = df.filter(df.G2 < 12)\n",
    "    for i in range(12):\n",
    "        reproved = reproved.withColumn(\"G2\", regexp_replace(\"G2\", \"{}\".format(i), \"0\"))\n",
    "    for i in range(12,20):\n",
    "        approved = approved.withColumn(\"G2\", regexp_replace(\"G2\", \"{}\".format(i), \"1\"))\n",
    "    df = approved.union(reproved)\n",
    "    df = df.withColumn('G2', df.G2.astype(\"int\"))\n",
    "\n",
    "    #Estableciendo umbral para el tercer periodo\n",
    "    df = df.withColumn('G3', df.G3.astype(\"int\"))\n",
    "    approved = df.filter(df.G3 >= 12)\n",
    "    reproved = df.filter(df.G3 < 12)\n",
    "    for i in range(12):\n",
    "        reproved = reproved.withColumn(\"G3\", regexp_replace(\"G3\", \"{}\".format(i), \"0\"))\n",
    "    for i in range(12,20):\n",
    "        approved = approved.withColumn(\"G3\", regexp_replace(\"G3\", \"{}\".format(i), \"1\"))\n",
    "        \n",
    "    #print('Número de estudiantes que rerobaron:', reproved.count())\n",
    "    #print('Número de estudiantes que aprobaron:', approved.count())\n",
    "    #Aquí obtuvimos 301 estudiantes reprobados y 348 estudiantes aprobados\n",
    "\n",
    "    df = approved.union(reproved)\n",
    "    df = df.withColumn('G3', df.G3.astype(\"int\"))\n",
    "\n",
    "    #df.count()\n",
    "    #df.show()\n",
    "    return(df)\n",
    "\n",
    "#----------------------------------ANALISIS-----------------------------------#\n",
    "\n",
    "#Visualización de las medidas de centralidad\n",
    "\n",
    "def describeData(df):\n",
    "    #información estadistica acerca de los datos \n",
    "    df.describe().toPandas()\n",
    "    df.toPandas().mode()\n",
    "\n",
    "def boxWhiskerPlot(df,headings):\n",
    "    #Diagramas de cajas y bigotes \n",
    "    for x in headings:\n",
    "        print(x)\n",
    "        plt.boxplot(df.toPandas()[x],vert = 0)\n",
    "        plt.show()\n",
    "\n",
    "def countAtypicValues(df):\n",
    "    #Analisis de los diagramas \n",
    "    #cantidad de datos atipicos\n",
    "\n",
    "    atypic_age_22=df.filter(df['age'] == 22).count()\n",
    "    atypic_p_status_0=df.filter(df['Pstatus'] == 0).count()#viven padres juntos o separados \n",
    "    atypic_travel_time_4=df.filter(df['traveltime'] == 4).count()#tiempo de la casa a el colegio\n",
    "    atypic_studytime_4=df.filter(df['studytime'] == 4).count()#tiempo de estudio\n",
    "    atypic_failures_1=df.filter(df['failures'] == 1).count()#número de fallos de clases anteriores\n",
    "    atypic_failures_2=df.filter(df['failures'] == 2).count()\n",
    "    atypic_failures_3=df.filter(df['failures'] == 3).count() \n",
    "    atypic_schoolsup_1=df.filter(df['schoolsup'] == 1).count()#apoyo educativo adicional\n",
    "    atypic_paid_1=df.filter(df['paid'] == 1).count()#clases extra pagadas dentro de la asignatura del curso (portugués)\n",
    "    atypic_nursery_0=df.filter(df['nursery'] == 0).count()#asistio a la guarderia\n",
    "    atypic_higher_0=df.filter(df['higher'] == 0).count()#piensa  cursar estudios superiores\n",
    "    atypic_internet_0=df.filter(df['internet'] == 0).count()\n",
    "    atypic_famrel_1=df.filter(df['famrel'] == 1).count()#calidad de las relaciones familiares\n",
    "    atypic_famrel_2=df.filter(df['famrel'] == 2).count()\n",
    "    atypic_freetime_1=df.filter(df['freetime'] == 1).count()#tiempo libre despues de la escuela\n",
    "    atypic_Dalc_4=df.filter(df['Dalc'] == 4).count()# consumo de alcohol entre semana\n",
    "    atypic_Dalc_5=df.filter(df['Dalc'] == 5).count()\n",
    "    atypic_absences=df.filter(df['absences'] > 16).count()#numero de ausencias escolares\n",
    "\n",
    "    print(\"atypic_age_22:\",atypic_age_22)\n",
    "    print(\"atypic_p_status_0:\",atypic_p_status_0)\n",
    "    print(\"atypic_travel_time_4:\",atypic_travel_time_4)\n",
    "    print(\"atypic_studytime_4:\",atypic_studytime_4)\n",
    "    print(\"atypic_failures_1:\",atypic_failures_1)\n",
    "    print(\"atypic_failures_2:\",atypic_failures_2)\n",
    "    print(\"atypic_failures_3:\",atypic_failures_3)\n",
    "    print(\"atypic_schoolsup_1:\",atypic_schoolsup_1)\n",
    "    print(\"atypic_paid_1:\",atypic_paid_1)\n",
    "    print(\"atypic_nursery_0:\",atypic_nursery_0)\n",
    "    print(\"atypic_higher_0:\",atypic_higher_0)\n",
    "    print(\"atypic_internet_0:\",atypic_internet_0)\n",
    "    print(\"atypic_famrel_1:\",atypic_famrel_1)\n",
    "    print(\"atypic_famrel_2:\",atypic_famrel_2)\n",
    "    print(\"atypic_freetime_1:\",atypic_freetime_1)\n",
    "    print(\"atypic_Dalc_4:\",atypic_Dalc_4)\n",
    "    print(\"atypic_Dalc_5:\",atypic_Dalc_5)\n",
    "    print(\"atypic_absences:\",atypic_absences)\n",
    "\n",
    "def dropAtypicValues(df):\n",
    "    \"\"\"\n",
    "    Eliminación de datos atipicos\n",
    "    Nota: Para esta fase establecimos que estabamos dispuestos a eliminar hasta un 10%\n",
    "    del total de los datos del dataset (649).\n",
    "    \"\"\"\n",
    "    df=df.filter(df['age'] != 22)\n",
    "    df=df.filter(df['traveltime'] != 4)\n",
    "    df=df.filter(df['absences'] <17)\n",
    "    df=df.filter(df['Dalc'] != 5)\n",
    "    #df.count()\n",
    "    #Al depurar los datos atípicos, terminamos con un total de 608 datos.\n",
    "    return(df)\n",
    "\n",
    "def dataBalancing(df):\n",
    "\n",
    "    #Mirar balance de los datos\n",
    "\n",
    "    approved = df.filter(df.G3 == 1)\n",
    "    reproved = df.filter(df.G3 == 0)\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"cantidad final de estudiantes aprobados\",approved.count())\n",
    "    print(\"cantidad final de estudiantes reprobados\",reproved.count())\n",
    "    \"\"\"\n",
    "\n",
    "    #Aplicar un balanceo de los datos reduciendo la clase mayorataria\n",
    "    approved=approved.sample(fraction=0.809,seed = 9403040)\n",
    "    #print( \"approved\",approved.count())\n",
    "    df = approved.union(reproved)\n",
    "\n",
    "    #print(df.dtypes,\"df.dtypes_dataBalancing\")\n",
    "\n",
    "    return(df)\n",
    "\n",
    "#-------------------------REGRESION LOGISTICA-----------------------------#\n",
    "#NOTA: En este modelo se intento variar el parametro de maxIter en ambos datasets,\n",
    "#pero este parametro no afectaba el desempeño del modelo en este caso.\n",
    "\n",
    "def logistic_Regression(df,trainingData,testData,maxIterValue,thresholdValue,familyValue):\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"logistic_Regression\")\n",
    "\n",
    "    lr = LogisticRegression(labelCol=\"G3\", featuresCol=\"features\",maxIter=maxIterValue,\n",
    "                                            threshold=thresholdValue, family=familyValue)\n",
    "\n",
    "   # Fit the model\n",
    "\n",
    "    model = lr.fit(trainingData)\n",
    "\n",
    "   # make predictions using our trained model\n",
    "\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    # estimate the accuracy of the prediction\n",
    "\n",
    "    multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"G3\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = multi_evaluator.evaluate(predictions)\n",
    "\n",
    "    multi_evaluator = multi_evaluator.setMetricName('precisionByLabel')\n",
    "    precision = multi_evaluator.evaluate(predictions)\n",
    "\n",
    "    multi_evaluator = multi_evaluator.setMetricName('recallByLabel')\n",
    "    recall = multi_evaluator.evaluate(predictions)\n",
    "    \n",
    "    multi_evaluator = multi_evaluator.setMetricName('f1')\n",
    "    f1_score = multi_evaluator.evaluate(predictions)\n",
    "\n",
    "    bin_evaluator = BinaryClassificationEvaluator(labelCol=\"G3\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "    area = bin_evaluator.evaluate(predictions)\n",
    "\n",
    "\n",
    "    print(\"Accuracy = {}\".format(accuracy))\n",
    "    print(\"Precision = {}\".format(precision))\n",
    "    print(\"Recall = {}\".format(recall))\n",
    "    print(\"F1 score = {}\".format(f1_score))\n",
    "    print(\"Area under ROC curve = {}\".format(area))\n",
    "\n",
    "    return (model)\n",
    "\n",
    "#----------------------------MACHINE LEARNING--------------------------------#\n",
    "\n",
    "#-----------------------------RANDOM FOREST----------------------------------#\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "def random_Forest(df,trainingData,testData,numTreesValue,maxDepthValue,featureSubsetStrategyValue):\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"random_Forest\")\n",
    "    rf = RandomForestClassifier(labelCol=\"G3\", featuresCol=\"features\", numTrees=numTreesValue,maxDepth=maxDepthValue,\n",
    "                                             featureSubsetStrategy=featureSubsetStrategyValue)\n",
    "\n",
    "    # Fit the model\n",
    "    model = rf.fit(trainingData)\n",
    "\n",
    "    # make predictions using our trained model\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    # estimate the accuracy of the prediction\n",
    "    multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"G3\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = multi_evaluator.evaluate(predictions)\n",
    "\n",
    "    multi_evaluator = multi_evaluator.setMetricName('precisionByLabel')\n",
    "    precision = multi_evaluator.evaluate(predictions)\n",
    "        \n",
    "    multi_evaluator = multi_evaluator.setMetricName('recallByLabel')\n",
    "    recall = multi_evaluator.evaluate(predictions)\n",
    "\n",
    "    multi_evaluator = multi_evaluator.setMetricName('f1')\n",
    "    f1_score = multi_evaluator.evaluate(predictions)\n",
    "    bin_evaluator = BinaryClassificationEvaluator(labelCol=\"G3\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "    area = bin_evaluator.evaluate(predictions)\n",
    "    \n",
    "    print(\"Accuracy = {}\".format(accuracy))\n",
    "    print(\"Precision = {}\".format(precision))\n",
    "    print(\"Recall = {}\".format(recall))\n",
    "    print(\"F1 score = {}\".format(f1_score))\n",
    "    print(\"Area under ROC curve = {}\".format(area))\n",
    "    \n",
    "    # print model summary\n",
    "    return (model)\n",
    "\n",
    "\n",
    "#--------------------VECTOR DE MÁQUINA DE SOPORTES------------------------#\n",
    "#NOTA: Este modelo cuenta con los siguientes párametros predeterminados:\n",
    "#      maxIter = 100\n",
    "#      threshold = 0.0\n",
    "#      aggregationDepth = 0.2\n",
    "#      regParam = 0.0\n",
    "#------------------------------Dataset 1----------------------------------#\n",
    "\n",
    "def svm(df,trainingData,testData,maxIterValue,regParamValue,depth,thresholdValue):\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"mvs\")\n",
    "            \n",
    "    svm = LinearSVC(labelCol=\"G3\", featuresCol=\"features\", maxIter=maxIterValue, regParam=regParamValue, aggregationDepth=depth, threshold=thresholdValue)\n",
    "\n",
    "    # Fit the model\n",
    "    model = svm.fit(trainingData)\n",
    "\n",
    "    # make predictions using our trained model\n",
    "\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    # estimate the accuracy of the prediction\n",
    "    #Métricas de evaluación\n",
    "    multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"G3\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = multi_evaluator.evaluate(predictions)\n",
    "\n",
    "    multi_evaluator = multi_evaluator.setMetricName('precisionByLabel')\n",
    "    precision = multi_evaluator.evaluate(predictions)\n",
    "\n",
    "    multi_evaluator = multi_evaluator.setMetricName('f1')\n",
    "    f1_score = multi_evaluator.evaluate(predictions)\n",
    "    \n",
    "    multi_evaluator = multi_evaluator.setMetricName('recallByLabel')\n",
    "    recall = multi_evaluator.evaluate(predictions)\n",
    "\n",
    "    bin_evaluator = BinaryClassificationEvaluator(labelCol=\"G3\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "    area = bin_evaluator.evaluate(predictions)\n",
    "    \n",
    "    print(\"Accuracy = {}\".format(accuracy))\n",
    "    print(\"Precision = {}\".format(precision))\n",
    "    print(\"Recall = {}\".format(recall))\n",
    "    print(\"F1 score = {}\".format(f1_score))\n",
    "    print(\"Area under ROC curve = {}\".format(area))\n",
    "    return (model)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    #Encabezado del dataframe\n",
    "    headings = ['school','sex','age','address','famsize','Pstatus','Medu','Fedu','Mjob',\n",
    "     'Fjob','reason','guardian','traveltime','studytime','failures','schoolsup',\n",
    "     'famsup','paid','activities','nursery','higher','internet','romantic',\n",
    "     'famrel','freetime','goout','Dalc','Walc','health','absences','G1','G2','G3']\n",
    "    spark = SparkSession.builder.appName(\"Student\").getOrCreate()\n",
    "\n",
    "    #Crear dataframe\n",
    "    df=spark.read.csv('student-por.csv',sep=';',header=True)\n",
    "    \n",
    "    \"\"\"\n",
    "    #Visualización del dataframe\n",
    "    df.show()\n",
    "\n",
    "    #Tamaño del dataset\n",
    "    print(df.count())\n",
    "    El dataframe tiene 649 registros\n",
    "\n",
    "    #Tipo de dato de cada variable\n",
    "    print(df.dtypes)\n",
    "    #NOTA:Todos los datos del dataframe inicial son de tipo string\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #Cantidad de datos nulos \n",
    "    #nullData(df)\n",
    "    #Reemplazar valores categoricos a numericos\n",
    "    df=categoricalToNumerical(df)\n",
    "\n",
    "    #Convertir los datos de string a int\n",
    "    df=stringToInt(df)\n",
    "\n",
    "    #Convertir variables categorica a numericas\n",
    "    df=approvedOrReproved(df)\n",
    "    \n",
    "    #Visualizar la correlación de las variables\n",
    "    #correlacion = correlation(df,headings)\n",
    "    #sns.heatmap(correlacion, square=True)\n",
    "\n",
    "    #describeData(df)\n",
    "    #boxWhiskerPlot(df,headings)\n",
    "\n",
    "    #countAtypicValues(df)\n",
    "\n",
    "    df=dropAtypicValues(df)\n",
    "\n",
    "    df=dataBalancing(df)\n",
    "\n",
    "    #------------------------CREACIÓN DE LOS DATASETS FINALES---------------------#\n",
    "\n",
    "    #Crear vectores assembler\n",
    "    vector1 = VectorAssembler(inputCols=['school','sex','age','address','famsize','Pstatus','Medu','Fedu','Mjob',\n",
    "        'Fjob','reason','guardian','traveltime','studytime','failures','schoolsup','famsup','paid','activities',\n",
    "        'nursery','higher','internet','romantic','famrel','freetime','goout','Dalc','Walc','health','absences'],\n",
    "        outputCol=\"features\")\n",
    "\n",
    "    #Adaptar los vectores al conjunto de datos\n",
    "\n",
    "    df1_temp = vector1.transform(df)\n",
    "    \n",
    "    #df_temp.show(5)\n",
    "    # get dataframe with all necessary data in the appropriate form\n",
    "\n",
    "    df1 = df1_temp.drop('school','sex','age','address','famsize','Pstatus','Medu','Fedu',\n",
    "        'Mjob','Fjob','reason','guardian','traveltime','studytime','failures','schoolsup',\n",
    "        'famsup','paid','activities','nursery','higher','internet','romantic',\n",
    "        'famrel','freetime','goout','Dalc','Walc','health','absences','G1','G2')\n",
    "        \n",
    "    #Partición de los dataframes\n",
    "\n",
    "    trainingData_1, testData_1= df1.randomSplit([0.7,0.3],seed=2102020)\n",
    "\n",
    "    print(\"Sin G1 y G2\")\n",
    "\n",
    "    #Modelo regresión logistica-dataset1 con el mejor desempeño\n",
    "    rl1_model1=logistic_Regression(df1,trainingData_1,testData_1,maxIterValue=50,thresholdValue=0.55,familyValue='binomial')\n",
    "\n",
    "    #Otros modelos de regresión logística evaluados para el dataset1\n",
    "    #rl2_model1=logistic_Regression(df1,trainingData_1,testData_1,maxIterValue=100,thresholdValue=0.5,familyValue='binomial')\n",
    "    #rl3_model1=logistic_Regression(df1,trainingData_1,testData_1,maxIterValue=100,thresholdValue=0.55,familyValue='binomial')\n",
    "    #rl4_model1=logistic_Regression(df1,trainingData_1,testData_1,maxIterValue=150,thresholdValue=0.55,familyValue='binomial')\n",
    "    #rl5_model1=logistic_Regression(df1,trainingData_1,testData_1,maxIterValue=50,thresholdValue=0.4,familyValue='binomial')\n",
    "    #rl6_model1=logistic_Regression(df1,trainingData_1,testData_1,maxIterValue=50,thresholdValue=0.5,familyValue='binomial')\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Sin G1 y G2\")\n",
    "    #Random forest dataset1 con mejor desempeño\n",
    "    rf1_model1=random_Forest(df1,trainingData_1,testData_1, numTreesValue=10,maxDepthValue=5,featureSubsetStrategyValue='onethird')\n",
    "\n",
    "    #Otros modelos\n",
    "    #rf2_model1=random_Forest(df1,trainingData_1,testData_1, numTreesValue=10,maxDepthValue=5,featureSubsetStrategyValue='sqrt')\n",
    "    #rf3_model1=random_Forest(df1,trainingData_1,testData_1, numTreesValue=10,maxDepthValue=5,featureSubsetStrategyValue='log2')\n",
    "    #rf4_model1=random_Forest(df1,trainingData_1,testData_1, numTreesValue=10,maxDepthValue=3,featureSubsetStrategyValue='sqrt')\n",
    "    #rf5_model1=random_Forest(df1,trainingData_1,testData_1, numTreesValue=10,maxDepthValue=8,featureSubsetStrategyValue='sqrt')\n",
    "\n",
    "    print(\"Sin G1 y G2\")\n",
    "    #Vector de maquinas de soporte con mejor desempeño para dataset1\n",
    "    mvs1_model1=svm(df1,trainingData_1,testData_1, maxIterValue =10, thresholdValue=0.5, depth = 2, regParamValue = 0.0)\n",
    "\n",
    "    #Otros modelos\n",
    "    #mvs2_model1=svm(df1,trainingData_1,testData_1,maxIterValue=100,thresholdValue = 0.0, depth = 2, regParamValue=0)\n",
    "    #mvs3_model1=svm(df1,trainingData_1,testData_1,maxIterValue=10,thresholdValue = 0.0, depth = 2, regParamValue=0)\n",
    "    #mvs4_model1=svm(df1,trainingData_1,testData_1,maxIterValue  = 100, thresholdValue = 0.0, depth = 2, regParamValue=0.1)\n",
    "    #mvs5_model1=svm(df1,trainingData_1,testData_1, maxIterValue =10, thresholdValue = 0.0, depth = 2,  regParamValue=0.1)\n",
    "    #mvs6_model1=svm(df1,trainingData_1,testData_1, maxIterValue =10, thresholdValue = 0.0, depth = 2, regParamValue = 0.0)\n",
    "    #mvs7_model1=svm(df1,trainingData_1,testData_1, maxIterValue =150, thresholdValue = 0.0, depth = 2, regParamValue = 0.0)\n",
    "    #mvs8_model1=svm(df1,trainingData_1,testData_1, maxIterValue  = 100, thresholdValue=0.5, depth = 2, regParamValue = 0.0)\n",
    "    #mvs9_model1=svm(df1,trainingData_1,testData_1, maxIterValue  = 100, thresholdValue = 0.0, depth=3, regParamValue = 0.0)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"#------------------------------------------------------------------------------------------#\")\n",
    "\n",
    "    #Crear vectores assembler\n",
    "\n",
    "    vector2 = VectorAssembler(inputCols=['school','sex','age','address','famsize','Pstatus','Medu','Fedu','Mjob',\n",
    "        'Fjob','reason','guardian','traveltime','studytime','failures','schoolsup',\n",
    "        'famsup','paid','activities','nursery','higher','internet','romantic',\n",
    "        'famrel','freetime','goout','Dalc','Walc','health','absences','G1'], outputCol=\"features\")\n",
    "\n",
    "    #Adaptar los vectores al conjunto de datos\n",
    "\n",
    "    df2_temp = vector2.transform(df)\n",
    "\n",
    "    #df2_temp.show(5)\n",
    "    # get dataframe with all necedf2_tempssary data in the appropriate form\n",
    "\n",
    "    df2 = df2_temp.drop('school','sex','age','address','famsize','Pstatus','Medu','Fedu','Mjob',\n",
    "        'Fjob','reason','guardian','traveltime','studytime','failures','schoolsup',\n",
    "        'famsup','paid','activities','nursery','higher','internet','romantic',\n",
    "        'famrel','freetime','goout','Dalc','Walc','health','absences','G1','G2')\n",
    "\n",
    "    #Partición de los dataframes\n",
    "\n",
    "    trainingData_2, testData_2= df2.randomSplit([0.7,0.3],seed=3102020)\n",
    "\n",
    "    print(\"Sin G2\")\n",
    "    #Modelo regresión logistica-dataset1 con el mejor desempeño\n",
    "    rl1_model2=logistic_Regression(df2,trainingData_2,testData_2,maxIterValue=100,thresholdValue=0.55,familyValue='binomial') \n",
    "\n",
    "    #Otros modelos de regresión logística evaluados para el dataset1\n",
    "    #rl2_model2=logistic_Regression(df2,trainingData_2,testData_2,maxIterValue=50,thresholdValue=0.55,familyValue='binomial')\n",
    "    #rl3_model2=logistic_Regression(df2,trainingData_2,testData_2,maxIterValue=100,thresholdValue=0.5,familyValue='binomial')\n",
    "    #rl4_model2=logistic_Regression(df2,trainingData_2,testData_2,maxIterValue=150,thresholdValue=0.55,familyValue='binomial')\n",
    "    #rl5_model2=logistic_Regression(df2,trainingData_2,testData_2,maxIterValue=50,thresholdValue=0.4,familyValue='binomial')\n",
    "    #rl6_model2=logistic_Regression(df2,trainingData_2,testData_2,maxIterValue=50,thresholdValue=0.5,familyValue='binomial')\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Sin G2\")\n",
    "\n",
    "    #Mejor modelo de random forest para el dataset 2\n",
    "    rf1_model2=random_Forest(df2,trainingData_2,testData_2, numTreesValue=10,maxDepthValue=5,featureSubsetStrategyValue='onethird')\n",
    "\n",
    "    #Otros modelos\n",
    "    #rf2_model2=random_Forest(df2,trainingData_2,testData_2, numTreesValue=10,maxDepthValue=5,featureSubsetStrategyValue='sqrt')\n",
    "    #rf3_model2=random_Forest(df2,trainingData_2,testData_2, numTreesValue=10,maxDepthValue=5,featureSubsetStrategyValue='log2')\n",
    "    #rf4_model2=random_Forest(df2,trainingData_2,testData_2, numTreesValue=10,maxDepthValue=3,featureSubsetStrategyValue='sqrt')\n",
    "    #rf5_model2=random_Forest(df2,trainingData_2,testData_2, numTreesValue=10,maxDepthValue=8,featureSubsetStrategyValue='sqrt')\n",
    "\n",
    "    print(\"Sin G2\")\n",
    "    mvs1_model2=svm(df2,trainingData_2,testData_2,maxIterValue=100,thresholdValue = 0.0, depth = 2, regParamValue=0)\n",
    "\n",
    "    #mvs2_model2=svm(df2,trainingData_2,testData_2, maxIterValue =10, thresholdValue=0.5, depth = 2, regParamValue = 0.0)\n",
    "    #mvs3_model2=svm(df2,trainingData_2,testData_2,maxIterValue=10,thresholdValue = 0.0, depth = 2, regParamValue=0)\n",
    "    #mvs4_model2=svm(df2,trainingData_2,testData_2,maxIterValue = 100, thresholdValue = 0.0, depth = 2, regParamValue=0.1)\n",
    "    #mvs5_model2=svm(df2,trainingData_2,testData_2, maxIterValue =10, thresholdValue = 0.0, depth = 2,  regParamValue=0.1)\n",
    "    #mvs6_model2=svm(df2,trainingData_2,testData_2, maxIterValue =10, thresholdValue = 0.0, depth = 2, regParamValue = 0.0)\n",
    "    #mvs7_model2=svm(df2,trainingData_2,testData_2, maxIterValue =150, thresholdValue = 0.0, depth = 2, regParamValue = 0.0)\n",
    "    #mvs8_model2=svm(df2,trainingData_2,testData_2, maxIterValue = 100, thresholdValue=0.5, depth = 2, regParamValue = 0.0)\n",
    "    #mvs9_model2=svm(df2,trainingData_2,testData_2, maxIterValue = 100, thresholdValue = 0.0, depth=3, regParamValue = 0.0)\n",
    "\n",
    "    #Finaliza la sesión de spark\n",
    "    spark.stop()\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
